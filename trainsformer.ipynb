{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import random_split, Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.4.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch Version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ëŒ€ë¶„ë¥˜ ì†Œë¶„ë¥˜       ìƒí™©  Set Nr.  ë°œí™”ì                            ì›ë¬¸  \\\n",
      "0  ë¹„ì¦ˆë‹ˆìŠ¤  íšŒì˜  ì˜ê²¬ êµí™˜í•˜ê¸°        1  A-1   ì´ë²ˆ ì‹ ì œí’ˆ ì¶œì‹œì— ëŒ€í•œ ì‹œì¥ì˜ ë°˜ì‘ì€ ì–´ë–¤ê°€ìš”?   \n",
      "1  ë¹„ì¦ˆë‹ˆìŠ¤  íšŒì˜  ì˜ê²¬ êµí™˜í•˜ê¸°        1  B-1    íŒë§¤ëŸ‰ì´ ì§€ë‚œë²ˆ ì œí’ˆë³´ë‹¤ ë¹ ë¥´ê²Œ ëŠ˜ê³  ìˆìŠµë‹ˆë‹¤.   \n",
      "2  ë¹„ì¦ˆë‹ˆìŠ¤  íšŒì˜  ì˜ê²¬ êµí™˜í•˜ê¸°        1  A-2  ê·¸ë ‡ë‹¤ë©´ ê³µì¥ì— ì—°ë½í•´ì„œ ì£¼ë¬¸ëŸ‰ì„ ë” ëŠ˜ë ¤ì•¼ê² ë„¤ìš”.   \n",
      "3  ë¹„ì¦ˆë‹ˆìŠ¤  íšŒì˜  ì˜ê²¬ êµí™˜í•˜ê¸°        1  B-2   ë„¤, ì œê°€ ì—°ë½í•´ì„œ ì£¼ë¬¸ëŸ‰ì„ 2ë°°ë¡œ ëŠ˜ë¦¬ê² ìŠµë‹ˆë‹¤.   \n",
      "4  ë¹„ì¦ˆë‹ˆìŠ¤  íšŒì˜  ì˜ê²¬ êµí™˜í•˜ê¸°        2  A-1   ì§€ë‚œ íšŒì˜ ë§ˆì§€ë§‰ì— ë…¼ì˜í–ˆë˜ ì•ˆê±´ì„ ë‹¤ì‹œ ë³¼ê¹Œìš”?   \n",
      "\n",
      "                                                 ë²ˆì—­ë¬¸  \n",
      "0  How is the market's reaction to the newly rele...  \n",
      "1  The sales increase is faster than the previous...  \n",
      "2  Then, we'll have to call the manufacturer and ...  \n",
      "3  Sure, I'll make a call and double the volume o...  \n",
      "4  Shall we take a look at the issues we discusse...  \n"
     ]
    }
   ],
   "source": [
    "data_path = \"./Korean-English/2_ëŒ€í™”ì²´.xlsx\"\n",
    "data = pd.read_excel(data_path)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 7)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data.loc[idx, 'ì›ë¬¸'], self.data.loc[idx, 'ë²ˆì—­ë¬¸']\n",
    "\n",
    "custom_DS = CustomDataset(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] PositionalEncoding  \n",
    "[2] MultiHeadAttention  \n",
    "[3] FeedForward  \n",
    "[4] EncoderLayer  \n",
    "[5] DecoderLayer  \n",
    "[6] Encoder  \n",
    "[7] Decoder  \n",
    "[8] Transformer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer.Models import Transformer\n",
    "from transformer.Optim import ScheduledOptim\n",
    "from utils.argument_parser import get_args\n",
    "import os\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_src(src, pad_idx):\n",
    "    src = src.transpose(0, 1)\n",
    "    return src\n",
    "\n",
    "\n",
    "def patch_trg(trg, pad_idx):\n",
    "    trg = trg.transpose(0, 1)\n",
    "    trg, gold = trg[:, :-1], trg[:, 1:].contiguous().view(-1)\n",
    "    return trg, gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = get_args()\n",
    "opt.cuda = not opt.no_cuda\n",
    "opt.d_word_vec = opt.d_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] The warmup steps may be not enough.\n",
      "(sz_b, warmup) = (2048, 4000) is the official setting.\n",
      "Using smaller batch w/o longer warmup may cause the warmup stage ends with only little data trained.\n"
     ]
    }
   ],
   "source": [
    "# For reproducibility\n",
    "if opt.seed is not None:\n",
    "    torch.manual_seed(opt.seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # torch.set_deterministic(True)\n",
    "    np.random.seed(opt.seed)\n",
    "    random.seed(opt.seed)\n",
    "\n",
    "if not opt.output_dir:\n",
    "    print('No experiment result will be saved.')\n",
    "\n",
    "if not os.path.exists(opt.output_dir):\n",
    "    os.makedirs(opt.output_dir)\n",
    "\n",
    "if opt.batch_size < 2048 and opt.n_warmup_steps <= 4000:\n",
    "    print('[Warning] The warmup steps may be not enough.\\n'\\\n",
    "            '(sz_b, warmup) = (2048, 4000) is the official setting.\\n'\\\n",
    "            'Using smaller batch w/o longer warmup may cause '\\\n",
    "            'the warmup stage ends with only little data trained.')\n",
    "\n",
    "device = torch.device('cuda' if opt.cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer.Constants as Constants\n",
    "\n",
    "# 1. ì „ì²´ ë¬¸ì¥ ëª¨ìœ¼ê¸°\n",
    "all_src_sentences = [custom_DS[i][0] for i in range(len(custom_DS))]\n",
    "all_trg_sentences = [custom_DS[i][1] for i in range(len(custom_DS))]\n",
    "\n",
    "# 2. Tokenize (ê³µë°± ê¸°ì¤€ìœ¼ë¡œ ë‹¨ì–´ ë¶„ë¦¬)\n",
    "src_tokens = [token for sent in all_src_sentences for token in sent.split()]\n",
    "trg_tokens = [token for sent in all_trg_sentences for token in sent.split()]\n",
    "\n",
    "# 3. Special tokens\n",
    "special_tokens = [Constants.PAD_WORD, Constants.BOS_WORD, Constants.EOS_WORD, Constants.UNK_WORD]\n",
    "\n",
    "# 4. Vocab ë§Œë“¤ê¸°\n",
    "src_vocab = {token: idx for idx, token in enumerate(special_tokens + sorted(set(src_tokens)))}\n",
    "trg_vocab = {token: idx for idx, token in enumerate(special_tokens + sorted(set(trg_tokens)))}\n",
    "\n",
    "# 5. idx2word ë§¤í•‘\n",
    "src_idx2word = {idx: word for word, idx in src_vocab.items()}\n",
    "trg_idx2word = {idx: word for word, idx in trg_vocab.items()}\n",
    "\n",
    "# 6. opt ì„¸íŒ…\n",
    "max_src_len = max(len(sent.split()) for sent in all_src_sentences)\n",
    "max_trg_len = max(len(sent.split()) for sent in all_trg_sentences)\n",
    "opt.max_token_seq_len = max(max_src_len, max_trg_len) + 2\n",
    "\n",
    "opt.src_pad_idx = src_vocab[Constants.PAD_WORD]\n",
    "opt.trg_pad_idx = trg_vocab[Constants.PAD_WORD]\n",
    "opt.src_vocab_size = len(src_vocab)\n",
    "opt.trg_vocab_size = len(trg_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê¸¸ì´: 100000\n",
      "ì˜ˆì‹œ: ['ì´ë²ˆ ì‹ ì œí’ˆ ì¶œì‹œì— ëŒ€í•œ ì‹œì¥ì˜ ë°˜ì‘ì€ ì–´ë–¤ê°€ìš”?', 'íŒë§¤ëŸ‰ì´ ì§€ë‚œë²ˆ ì œí’ˆë³´ë‹¤ ë¹ ë¥´ê²Œ ëŠ˜ê³  ìˆìŠµë‹ˆë‹¤.', 'ê·¸ë ‡ë‹¤ë©´ ê³µì¥ì— ì—°ë½í•´ì„œ ì£¼ë¬¸ëŸ‰ì„ ë” ëŠ˜ë ¤ì•¼ê² ë„¤ìš”.', 'ë„¤, ì œê°€ ì—°ë½í•´ì„œ ì£¼ë¬¸ëŸ‰ì„ 2ë°°ë¡œ ëŠ˜ë¦¬ê² ìŠµë‹ˆë‹¤.', 'ì§€ë‚œ íšŒì˜ ë§ˆì§€ë§‰ì— ë…¼ì˜í–ˆë˜ ì•ˆê±´ì„ ë‹¤ì‹œ ë³¼ê¹Œìš”?']\n",
      "['ì´ë²ˆ', 'ì‹ ì œí’ˆ', 'ì¶œì‹œì—', 'ëŒ€í•œ', 'ì‹œì¥ì˜', 'ë°˜ì‘ì€', 'ì–´ë–¤ê°€ìš”?', 'íŒë§¤ëŸ‰ì´', 'ì§€ë‚œë²ˆ', 'ì œí’ˆë³´ë‹¤']\n",
      "ì „ì²´ ë‹¨ì–´ ìˆ˜: 779,541\n",
      "ê³ ìœ  ë‹¨ì–´ ìˆ˜: 117,299\n",
      "0: <blank> â†’ 0\n",
      "1: <s> â†’ 1\n",
      "2: </s> â†’ 2\n",
      "3: <unk> â†’ 3\n",
      "4: \"êµ­ì–´í•™ â†’ 4\n",
      "5: \"ë‚˜ëŠ” â†’ 5\n",
      "0: <blank> â†’ 0\n",
      "1: <s> â†’ 1\n",
      "2: </s> â†’ 2\n",
      "3: <unk> â†’ 3\n",
      "4: \"Animal â†’ 4\n",
      "5: \"Be â†’ 5\n"
     ]
    }
   ],
   "source": [
    "print(\"ê¸¸ì´:\", len(all_src_sentences))\n",
    "print(\"ì˜ˆì‹œ:\", all_src_sentences[:5])\n",
    "\n",
    "print(src_tokens[:10])  # ì•ì˜ 10ê°œ ë‹¨ì–´ë§Œ í™•ì¸\n",
    "print(f\"ì „ì²´ ë‹¨ì–´ ìˆ˜: {len(src_tokens):,}\")\n",
    "print(f\"ê³ ìœ  ë‹¨ì–´ ìˆ˜: {len(set(src_tokens)):,}\")\n",
    "\n",
    "for i, (token, idx) in enumerate(src_vocab.items()):\n",
    "    print(f\"{i}: {token} â†’ {idx}\")\n",
    "    if i >= 5:  # ì•ì—ì„œ 5ê°œë§Œ ì¶œë ¥\n",
    "        break\n",
    "\n",
    "for i, (token, idx) in enumerate(trg_vocab.items()):\n",
    "    print(f\"{i}: {token} â†’ {idx}\")\n",
    "    if i >= 5:  # ì•ì—ì„œ 5ê°œë§Œ ì¶œë ¥\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. split\n",
    "total_size = len(custom_DS)\n",
    "train_size = int(total_size * 0.8)\n",
    "val_size = int(total_size * 0.1)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(custom_DS, [train_size, val_size, test_size])\n",
    "\n",
    "# 2. collate_fn\n",
    "def collate_fn(batch):\n",
    "    src_batch, trg_batch = zip(*batch)\n",
    "    \n",
    "    src_indices = []\n",
    "    trg_indices = []\n",
    "    for src_sent, trg_sent in zip(src_batch, trg_batch):\n",
    "        src = [src_vocab.get(token, src_vocab[Constants.UNK_WORD]) for token in src_sent.split()]\n",
    "        trg = [trg_vocab.get(token, trg_vocab[Constants.UNK_WORD]) for token in trg_sent.split()]\n",
    "\n",
    "        src = [src_vocab[Constants.BOS_WORD]] + src + [src_vocab[Constants.EOS_WORD]]\n",
    "        trg = [trg_vocab[Constants.BOS_WORD]] + trg + [trg_vocab[Constants.EOS_WORD]]\n",
    "\n",
    "        src_indices.append(torch.tensor(src, dtype=torch.long))\n",
    "        trg_indices.append(torch.tensor(trg, dtype=torch.long))\n",
    "\n",
    "    src_padded = torch.nn.utils.rnn.pad_sequence(src_indices, batch_first=True, padding_value=src_vocab[Constants.PAD_WORD])\n",
    "    trg_padded = torch.nn.utils.rnn.pad_sequence(trg_indices, batch_first=True, padding_value=trg_vocab[Constants.PAD_WORD])\n",
    "\n",
    "    return src_padded, trg_padded\n",
    "\n",
    "# 3. DataLoader\n",
    "train_DL = DataLoader(train_dataset, batch_size=opt.batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_DL = DataLoader(val_dataset, batch_size=opt.batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_DL = DataLoader(test_dataset, batch_size=opt.batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    opt.src_vocab_size,\n",
    "    opt.trg_vocab_size,\n",
    "    src_pad_idx=opt.src_pad_idx,\n",
    "    trg_pad_idx=opt.trg_pad_idx,\n",
    "    trg_emb_prj_weight_sharing=opt.proj_share_weight,\n",
    "    emb_src_trg_weight_sharing=opt.embs_share_weight,\n",
    "    d_k=opt.d_k,\n",
    "    d_v=opt.d_v,\n",
    "    d_model=opt.d_model,\n",
    "    d_word_vec=opt.d_word_vec,\n",
    "    d_inner=opt.d_inner_hid,\n",
    "    n_layers=opt.n_layers,\n",
    "    n_head=opt.n_head,\n",
    "    dropout=opt.dropout,\n",
    "    scale_emb_or_prj=opt.scale_emb_or_prj).to(device)\n",
    "\n",
    "optimizer = ScheduledOptim(\n",
    "        optim.Adam(transformer.parameters(), betas=(0.9, 0.98), eps=1e-09),\n",
    "        opt.lr_mul, opt.d_model, opt.n_warmup_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def patch_src(src, pad_idx):\n",
    "    return src\n",
    "\n",
    "\n",
    "def patch_trg(trg, pad_idx):\n",
    "    trg, gold = trg[:, :-1], trg[:, 1:].contiguous().view(-1)\n",
    "    return trg, gold\n",
    "\n",
    "\n",
    "def cal_performance(pred, gold, pad_idx):\n",
    "    gold = gold.contiguous().view(-1)\n",
    "    pred = pred.view(-1, pred.size(-1))\n",
    "    \n",
    "    non_pad_mask = gold.ne(pad_idx)\n",
    "    n_correct = pred.max(1)[1].eq(gold).masked_select(non_pad_mask).sum().item()\n",
    "    n_word = non_pad_mask.sum().item()\n",
    "\n",
    "    loss = F.cross_entropy(pred, gold, ignore_index=pad_idx, reduction='sum')\n",
    "    return loss, n_correct, n_word\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, opt, device):\n",
    "    ''' í•œ epoch ë™ì•ˆ í•™ìŠµí•˜ëŠ” í•¨ìˆ˜ (tqdm ì¶”ê°€ ë²„ì „) '''\n",
    "    model.train()\n",
    "    total_loss, n_word_total, n_word_correct = 0, 0, 0\n",
    "\n",
    "    desc = '  - (Training)   '\n",
    "    for src_seq, trg_seq in tqdm(train_loader, mininterval=2, desc=desc, leave=False):\n",
    "       \n",
    "        src_seq = patch_src(src_seq, opt.src_pad_idx).to(device)\n",
    "        trg_seq, gold = map(lambda x: x.to(device), patch_trg(trg_seq, opt.trg_pad_idx))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(src_seq, trg_seq)\n",
    "\n",
    "        loss, n_correct, n_word = cal_performance(pred, gold, opt.trg_pad_idx)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step_and_update_lr()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        n_word_total += n_word\n",
    "        n_word_correct += n_correct\n",
    "\n",
    "    loss_per_word = total_loss / n_word_total\n",
    "    accuracy = n_word_correct / n_word_total\n",
    "    return loss_per_word, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_epoch(model, val_loader, opt, device):\n",
    "    ''' í•œ epoch ë™ì•ˆ ê²€ì¦í•˜ëŠ” í•¨ìˆ˜ '''\n",
    "    model.eval()\n",
    "    total_loss, n_word_total, n_word_correct = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src_seq, trg_seq in val_loader:\n",
    "\n",
    "            src_seq = patch_src(src_seq, opt.src_pad_idx).to(device)\n",
    "            trg_seq, gold = map(lambda x: x.to(device), patch_trg(trg_seq, opt.trg_pad_idx))\n",
    "\n",
    "            pred = model(src_seq, trg_seq)\n",
    "            loss, n_correct, n_word = cal_performance(pred, gold, opt.trg_pad_idx)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            n_word_total += n_word\n",
    "            n_word_correct += n_correct\n",
    "\n",
    "    loss_per_word = total_loss / n_word_total\n",
    "    accuracy = n_word_correct / n_word_total\n",
    "    return loss_per_word, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "for epoch_i in range(opt.epoch):\n",
    "    print(f'[ Epoch {epoch_i} ]')\n",
    "\n",
    "    start = time.time()\n",
    "    train_loss, train_accu = train_epoch(transformer, train_DL, optimizer, opt, device)\n",
    "    print('  - (Training)   loss: {:.5f}, accuracy: {:.3f} %, elapse: {:.3f} min'.format(\n",
    "        train_loss, 100*train_accu, (time.time()-start)/60))\n",
    "\n",
    "    start = time.time()\n",
    "    val_loss, val_accu = eval_epoch(transformer, val_DL, opt, device)\n",
    "    print('  - (Validation) loss: {:.5f}, accuracy: {:.3f} %, elapse: {:.3f} min'.format(\n",
    "        val_loss, 100*val_accu, (time.time()-start)/60))\n",
    "\n",
    "    # ëª¨ë¸ ì €ì¥\n",
    "    if epoch_i % 20 == 0:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch_i,\n",
    "            'model': transformer.state_dict(),\n",
    "            'opt': opt,\n",
    "            'src_vocab': src_vocab,\n",
    "            'trg_vocab': trg_vocab\n",
    "        }\n",
    "        torch.save(checkpoint, f'{opt.output_dir}/transformer_epoch{epoch_i}.chkpt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformer.Constants as C  # PAD/BOS/EOS\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1. Causal mask  (B,L) â†’ (B,L,L)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def causal_mask_bf(seq_BL: torch.Tensor) -> torch.Tensor:\n",
    "    B, L = seq_BL.size()\n",
    "    # True == masked  (upper-triangular ì œì™¸)\n",
    "    mask = torch.triu(\n",
    "        torch.ones(L, L, dtype=torch.bool, device=seq_BL.device), diagonal=1\n",
    "    )\n",
    "    return mask.unsqueeze(0).expand(B, -1, -1)      # (B,L,L)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2. Greedy ë””ì½”ë”  (ëª¨ë¸ë„ batch-first)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "@torch.no_grad()\n",
    "def greedy_decode_bf(model, src_BL, opt, device):\n",
    "    \"\"\"\n",
    "    src_BL : (B, L)\n",
    "    ë°˜í™˜     : List[List[int]]  (ë°°ì¹˜ë³„ ì˜ˆì¸¡ í† í° ì‹œí€€ìŠ¤)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    B, _ = src_BL.size()\n",
    "\n",
    "    # â”€â”€ Encoder â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    src_mask = (src_BL != opt.src_pad_idx).unsqueeze(1)     # (B,1,L)\n",
    "    enc_out, *_ = model.encoder(src_BL, src_mask)           # (B,L,d)\n",
    "\n",
    "    # â”€â”€ ë””ì½”ë” ë°˜ë³µ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    ys = torch.full((B, 1), trg_vocab[C.BOS_WORD],\n",
    "                    dtype=torch.long, device=device)\n",
    "\n",
    "    for _ in range(opt.max_token_seq_len):\n",
    "        tgt_mask = (ys != opt.trg_pad_idx).unsqueeze(1) & causal_mask_bf(ys)\n",
    "        dec_out, *_ = model.decoder(ys, tgt_mask, enc_out, src_mask)  # (B,L',d)\n",
    "\n",
    "        logits   = model.trg_word_prj(dec_out)[:, -1]   # (B,V)\n",
    "        next_tok = logits.argmax(dim=-1, keepdim=True) # (B,1)\n",
    "        ys       = torch.cat([ys, next_tok], dim=1)\n",
    "\n",
    "        if (next_tok == trg_vocab[C.EOS_WORD]).all():\n",
    "            break\n",
    "\n",
    "    # EOS ì•ê¹Œì§€ ì˜ë¼ BOS ì œì™¸\n",
    "    results = []\n",
    "    for row in ys:\n",
    "        seq = row.tolist()\n",
    "        if C.EOS_WORD in seq:\n",
    "            seq = seq[1:seq.index(C.EOS_WORD)]\n",
    "        else:\n",
    "            seq = seq[1:]\n",
    "        results.append(seq)\n",
    "    return results\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3. Beam-search Translator (Batch-First)\n",
    "#    * Translator êµ¬í˜„ì´ batch-first ë²„ì „ì„ ì´ë¯¸ ì§€ì›í•œë‹¤ê³  ê°€ì •\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "beam_trans = Translator(\n",
    "    model=transformer,\n",
    "    beam_size=opt.beam_size,\n",
    "    max_seq_len=opt.max_token_seq_len,\n",
    "    src_pad_idx=opt.src_pad_idx,\n",
    "    trg_pad_idx=opt.trg_pad_idx,\n",
    "    trg_bos_idx=trg_vocab[C.BOS_WORD],\n",
    "    trg_eos_idx=trg_vocab[C.EOS_WORD],\n",
    ").to(device)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 4. ìƒ˜í”Œ 5ê°œ ë²ˆì—­\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\n[Sample Translation Results]\")\n",
    "\n",
    "pad, bos, eos = (\n",
    "    src_vocab[C.PAD_WORD],\n",
    "    trg_vocab[C.BOS_WORD],\n",
    "    trg_vocab[C.EOS_WORD],\n",
    ")\n",
    "\n",
    "for src_B_L, trg_B_L in test_DL:      # (B,L)\n",
    "    src_B_L, trg_B_L = src_B_L.to(device), trg_B_L.to(device)\n",
    "\n",
    "    for i in range(min(5, src_B_L.size(0))):\n",
    "        src_seq = src_B_L[i:i+1]     # (1,L)\n",
    "        trg_seq = trg_B_L[i:i+1]\n",
    "\n",
    "        # ë¬¸ìì—´ ë³€í™˜\n",
    "        src_txt  = [src_idx2word[t.item()] for t in src_seq[0] if t.item() != pad]\n",
    "        gold_txt = [trg_idx2word[t.item()] for t in trg_seq[0]\n",
    "                    if t.item() not in (pad, bos, eos)]\n",
    "\n",
    "        # Greedy\n",
    "        g_ids = greedy_decode_bf(transformer, src_seq, opt, device)[0]\n",
    "        g_txt = [trg_idx2word[t] for t in g_ids]\n",
    "\n",
    "        # Beam\n",
    "        b_ids = beam_trans.translate_sentence(src_seq)\n",
    "        b_txt = [trg_idx2word[t] for t in b_ids if t != eos]\n",
    "\n",
    "        print(f\"ğŸ”¹ Source                    : {' '.join(src_txt)}\")\n",
    "        print(f\"ğŸ”¸ Prediction (Greedy)       : {' '.join(g_txt)}\")\n",
    "        print(f\"ğŸŒŸ Prediction (Beam-{opt.beam_size}) : {' '.join(b_txt)}\")\n",
    "        print(f\"âœ… Ground-Truth              : {' '.join(gold_txt)}\")\n",
    "        print(\"-\" * 100)\n",
    "    break   # ì²« ë°°ì¹˜ 5ê°œë§Œ\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
